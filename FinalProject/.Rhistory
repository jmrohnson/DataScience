beer$normABV <- beer$ABV/max(beer$ABV)
head(beer)
beer$ABV
max(beer$ABV)
?max
max(beer$ABV[,])
max(beer$ABV[.])
max(beer$ABV)
sapplay(beer$ABV, max)
sapply(beer$ABV, max)
max(beer$ABV)
max(beer$ABV0:nrow(beer$ABV))
max(beer$ABV[0:nrow(beer$ABV)])
beer$ABV[0:nrow(beer$ABV)]
beer$ABV[0]
beer$ABV[1]
beer$ABV[3]
beer$ABV[10]
head(beer)
beer$ABV[5]
beer$ABV[1:nrow(beer$ABV)]
beer$ABV[1:5]
max(beer$ABV)
beer$ABV
max(beer$ABV, na.rm)
max(beer$ABV)[beer$ABV != NA]
max(beer$ABV)[beer$ABV != "NA""]
)
max(beer$ABV[beer$ABV != "NA])
max(beer$ABV[beer$ABV != "NA"])
beer$ABV[beer$ABV != "NA"]
apply(beer$ABV, function(x) if (x == "NA") {NULL})
apply(beer, ABV, function(x) if (x == "NA") {NULL})
head(beer)
apply(beer, 5, function(x) if (x == "NA") {NULL})
apply(beer, 5, function(x) if (x == "NA") {return NULL})
max(beer$ABV, na.rm = TRUE)
beer <- read.csv('http://www-958.ibm.com/software/analytics/manyeyes/datasets/af-er-beer-dataset/versions/1.txt', header=TRUE, sep='\t')
head(beer)
#This data is a beer ratings dataset.  We want to turn this into a classification problem, to label beers as GOOD or BAD.  How do we define those?  In this case we are going to say the top-rated beers are GOOD are others are BAD.
summary(beer$WR)
beer$good <- (beer$WR > 4.3)
#We also want to build some relevant features, things we might think relate to that.  We could use the 'Type' field, but we saw that that field was fairly granular, so let's extract some large categories from it.
beer$Ale <- grepl('Ale', beer$Type)
beer$IPA <- grepl('IPA', beer$Type)
beer$Stout <- grepl('Stout', beer$Type)
beer$Lager <- grepl('Lager', beer$Type)
beer$normABV <- beer$ABV/max(beer$ABV, na.rm=T)
head(beer)
#Let's use these elements to see if we can classify our beers as GOOD or BAD.
model <- glm(good ~ Ale + Stout + IPA + Lager, data=beer, family='binomial')
#The new value here is `family`.  This defines how to interpret our output variable. In linear regressions we interpreted as Gaussian or Normal, which is the default family.
#Next, we'll want to build training and test sets to see how well we can predict.
train.idx <- sample(1:nrow(beer), .7*nrow(beer))
training <- beer[train.idx,]
test <- beer[-train.idx,]
model <- glm(good ~ Ale + Stout + IPA + Lager, data=training, family='binomial')
#We can build a model on the training set and predict on the test, but how do we measure success?
#One measure is accuracy, another precision.  R has these built into the ROCR package.
install.packages('ROCR')
library('ROCR')
install.packages("ROCR")
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf <- performance(pred, measure='prec') #What % of the elements I predicted to be in the class actually?
perf <- performance(pred, measure='recall') #What % of the elements that are in class, did I predict to be in this class?
perf <- performance(pred, measure='f') #F-measure a balance between them
perf <- performance(pred, measure='auc') #Area Under the Curve, another way to balance them
library('ROCR')
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf <- performance(pred, measure='prec') #What % of the elements I predicted to be in the class actually?
perf <- performance(pred, measure='recall') #What % of the elements that are in class, did I predict to be in this class?
perf <- performance(pred, measure='f') #F-measure a balance between them
perf <- performance(pred, measure='recall') #What % of the elements that are in class, did I predict to be in this class?
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf <- performance(pred, measure='prec') #What % of the elements I predicted to be in the class actually?
perf <- performance(pred, measure='recall') #What % of the elements that are in class, did I predict to be in this class?
model <- glm(good ~ Ale + Stout + IPA + Lager + normABV, data=training, family='binomial')
model
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf
perf <- performance(pred, measure='f') #F-measure a balance between them
perf
fix(perf)
fix(perf)
fix(perf)
perf <- NULL
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf
library('ROCR')
#Let's load up some data of yeasty goodness
beer <- read.csv('http://www-958.ibm.com/software/analytics/manyeyes/datasets/af-er-beer-dataset/versions/1.txt', header=TRUE, sep='\t')
head(beer)
#This data is a beer ratings dataset.  We want to turn this into a classification problem, to label beers as GOOD or BAD.  How do we define those?  In this case we are going to say the top-rated beers are GOOD are others are BAD.
summary(beer$WR)
beer$good <- (beer$WR > 4.3)
#We also want to build some relevant features, things we might think relate to that.  We could use the 'Type' field, but we saw that that field was fairly granular, so let's extract some large categories from it.
beer$Ale <- grepl('Ale', beer$Type)
beer$IPA <- grepl('IPA', beer$Type)
beer$Stout <- grepl('Stout', beer$Type)
beer$Lager <- grepl('Lager', beer$Type)
beer$normABV <- beer$ABV/max(beer$ABV, na.rm=T)
head(beer)
#Let's use these elements to see if we can classify our beers as GOOD or BAD.
model <- glm(good ~ Ale + Stout + IPA + Lager, data=beer, family='binomial')
#The new value here is `family`.  This defines how to interpret our output variable. In linear regressions we interpreted as Gaussian or Normal, which is the default family.
#Next, we'll want to build training and test sets to see how well we can predict.
train.idx <- sample(1:nrow(beer), .7*nrow(beer))
training <- beer[train.idx,]
test <- beer[-train.idx,]
model <- glm(good ~ Ale + Stout + IPA + Lager + normABV, data=training, family='binomial')
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf
?performance
library('ROCR')
#Let's load up some data of yeasty goodness
beer <- read.csv('http://www-958.ibm.com/software/analytics/manyeyes/datasets/af-er-beer-dataset/versions/1.txt', header=TRUE, sep='\t')
head(beer)
#This data is a beer ratings dataset.  We want to turn this into a classification problem, to label beers as GOOD or BAD.  How do we define those?  In this case we are going to say the top-rated beers are GOOD are others are BAD.
summary(beer$WR)
beer$good <- (beer$WR > 4.3)
#We also want to build some relevant features, things we might think relate to that.  We could use the 'Type' field, but we saw that that field was fairly granular, so let's extract some large categories from it.
beer$Ale <- grepl('Ale', beer$Type)
beer$IPA <- grepl('IPA', beer$Type)
beer$Stout <- grepl('Stout', beer$Type)
beer$Lager <- grepl('Lager', beer$Type)
beer$normABV <- beer$ABV/max(beer$ABV, na.rm=T)
head(beer)
#Let's use these elements to see if we can classify our beers as GOOD or BAD.
model <- glm(good ~ Ale + Stout + IPA + Lager, data=beer, family='binomial')
#The new value here is `family`.  This defines how to interpret our output variable. In linear regressions we interpreted as Gaussian or Normal, which is the default family.
#Next, we'll want to build training and test sets to see how well we can predict.
train.idx <- sample(1:nrow(beer), .7*nrow(beer))
training <- beer[train.idx,]
test <- beer[-train.idx,]
model <- glm(good ~ Ale + Stout + IPA + Lager , data=training, family='binomial')
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='acc') #Simple accuracy, what % were right?
perf
perf <- performance(pred, measure='recall') #What % of the elements that are in class, did I predict to be in this class?
perf
perf <- performance(pred, measure='auc') #Area Under the Curve, another way to balance them
perf
perf <- performance(pred, measure='f') #F-measure a balance between them
perf
per$y.values
perf$y.values
perf <- performance(pred, measure='prec') #What % of the elements I predicted to be in the class actually?
perf
model <- glm(good ~ Ale + Stout + IPA + Lager + norm$ABV, data=training, family='binomial')
beer$normABV <- beer$ABV/max(beer$ABV, na.rm=T)
model <- glm(good ~ Ale + Stout + IPA + Lager + normABV, data=training, family='binomial')
perf <- performance(pred, measure='prec') #What % of the elements I predicted to be in the class actually?
test.predict <- predict.glm(model,test)
pred <- prediction(test.predict, test$good)
perf <- performance(pred, measure='prec') #What % of the elements I predicted to be in the class actually?
perf
perf <- performance(pred, measure='auc') #Area Under the Curve, another way to balance them
perf
binom.test(450,985, 3498/6946)
binom.test(450,985, 3498/6946, less)
binom.test(450,985, 3498/6946, less)
binom.test(450,985, 3498/6946, 'less')
binom.test(450,985, 3498/6946, 'greater')
install.packages("rattle")
install.packages("NbClust")
library(stats)
library(ggplot2)
set.seed(1)
# for our first example, let's create some synthetic easy-to-cluster data
d <- data.frame()
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(2, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(3, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(4, each=20))))
rnorm(20, 0, 0.1)
rep(1, each=20)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d -- easy clusters')
result1 <- kmeans(d[,1:2], 4)
# here are the results...note the algorithm found clusters whose means are close to the true means of our synthetic clusters
result1
d$cluster1 <- as.factor(result1$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster1)) + ggtitle('kmeans result1 -- success!\n(k=4)')
result2 <- kmeans(d[,1:2], 4)
# notice that the fit got worse!
# (eg, large decrease in between_SS / total_SS...also cluster means are not as good as before)
result2
View(d)
View(d)
d <- data.frame()
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(2, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(3, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(4, each=20))))
# have a look...this looks easy enough
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d -- easy clusters')
# perform clustering
result1 <- kmeans(d[,1:2], 4)
# here are the results...note the algorithm found clusters whose means are close to the true means of our synthetic clusters
result1
# plot results...we are looking good
d$cluster1 <- as.factor(result1$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster1)) + ggtitle('kmeans result1 -- success!\n(k=4)')
# suppose we repeat these steps...what do you expect to happen?
result2 <- kmeans(d[,1:2], 4)
# notice that the fit got worse!
# (eg, large decrease in between_SS / total_SS...also cluster means are not as good as before)
result2
d$cluster2 <- as.factor(result2$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster2)) + ggtitle('kmeans result2 -- trouble\n(k=4)')
d <- data.frame()
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(2, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(3, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(4, each=20))))
# have a look...this looks easy enough
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d -- easy clusters')
# perform clustering
result1 <- kmeans(d[,1:2], 4)
# here are the results...note the algorithm found clusters whose means are close to the true means of our synthetic clusters
result1
# plot results...we are looking good
d$cluster1 <- as.factor(result1$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster1)) + ggtitle('kmeans result1 -- success!\n(k=4)')
# suppose we repeat these steps...what do you expect to happen?
result2 <- kmeans(d[,1:2], 4)
# notice that the fit got worse!
# (eg, large decrease in between_SS / total_SS...also cluster means are not as good as before)
result2
# and this scatterplot shows that something is obviously not right...what happened?
d$cluster2 <- as.factor(result2$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster2)) + ggtitle('kmeans result2 -- trouble\n(k=4)')
# this instability is a result of the random initial seeds that the clustering algorithm uses
# if two initial seeds begin in the same cluster, then the algorithm will have difficulty finding all the clusters
# (in particular, the cluster which doesn't contain an initial seed will be difficult to identify)
# (note that in any case, the algorithm will still return exactly as many clusters as you asked it to!)
# so how can we create a more stable clustering fit? by repeating the fit several times and taking an average
# (this is effectively an ensemble clustering technique...we will talk about ensemble methods in more detail later)
result3 <- kmeans(d[,1:2], 4, nstart=10)
d$cluster3 <- as.factor(result3$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster3)) + ggtitle('kmeans result3 -- stable convergence\n(k=4, nstart=10)')
#
# what happens if we introduce a new length scale into the problem? how many clusters are in the dataset now?
d2 <- rbind(d[,1:3], data.frame(x=1000+rnorm(20,0,50), y=1000+rnorm(20,0,50), label=as.factor(rep(5, each=20))))
ggplot(d2, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d2 -- multiple length scales')
# as you can see, things go haywire...recall that clustering results are kind of a heuristic
# (in particular, not invariant to a change in units!)
result4 <- kmeans(d2[,1:2], 5, nstart=10)
d2$cluster4 <- as.factor(result4$cluster)
ggplot(d2, aes(x=x, y=y)) + geom_point(aes(colour=cluster4)) + ggtitle('kmeans result4 -- trouble\n(k=5, nstart=10)')
#
# now let's try k-means clustering with the iris dataset
iris.result <- kmeans(iris[,1:4], 3)
# look at clustering results...you can already tell something is up
iris.result$cluster
# combine clustering results with input data (as factor)
# let's look at the scatterplots of clustering results & true labels together (using package gridExtra)
# first install this guy
install.packages('gridExtra')
library(gridExtra)
# now create our two scatterplots...note that ggplot returns an *object* which can be stored!
iris2 <- cbind(iris, cluster=as.factor(iris.result$cluster))
p1 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=cluster)) + ggtitle('clustering results')
p2 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=Species)) + ggtitle('true labels')
# so what is going on here?
grid.arrange(p1, p2)
# answer: the iris dataset is not linearly separable!
# we will talk later about one way to deal with this (eg, using nonlinear classifiers)
# we will talk later about one way to deal with this (eg, using nonlinear classifiers)
set.seed(1)
# for our first example, let's create some synthetic easy-to-cluster data
d <- data.frame()
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(2, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(3, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(4, each=20))))
# have a look...this looks easy enough
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d -- easy clusters')
# perform clustering
result1 <- kmeans(d[,1:2], 4)
# here are the results...note the algorithm found clusters whose means are close to the true means of our synthetic clusters
result1
# plot results...we are looking good
d$cluster1 <- as.factor(result1$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster1)) + ggtitle('kmeans result1 -- success!\n(k=4)')
# suppose we repeat these steps...what do you expect to happen?
result2 <- kmeans(d[,1:2], 4)
# notice that the fit got worse!
# (eg, large decrease in between_SS / total_SS...also cluster means are not as good as before)
result2
# and this scatterplot shows that something is obviously not right...what happened?
d$cluster2 <- as.factor(result2$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster2)) + ggtitle('kmeans result2 -- trouble\n(k=4)')
# this instability is a result of the random initial seeds that the clustering algorithm uses
# if two initial seeds begin in the same cluster, then the algorithm will have difficulty finding all the clusters
# (in particular, the cluster which doesn't contain an initial seed will be difficult to identify)
# (note that in any case, the algorithm will still return exactly as many clusters as you asked it to!)
# so how can we create a more stable clustering fit? by repeating the fit several times and taking an average
# (this is effectively an ensemble clustering technique...we will talk about ensemble methods in more detail later)
result3 <- kmeans(d[,1:2], 4, nstart=10)
d$cluster3 <- as.factor(result3$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster3)) + ggtitle('kmeans result3 -- stable convergence\n(k=4, nstart=10)')
??kmeans
iris2 <- cbind(iris, cluster=as.factor(iris.result$cluster))
p1 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=cluster)) + ggtitle('clustering results')
p2 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=Species)) + ggtitle('true labels')
# so what is going on here?
grid.arrange(p1, p2)
install.packages("rattle")
install.packages("NbClust")
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
data(wine, package="rattle")
head(wine)
df <- scale(wine[-1])
#Determine number of clusters
wssplot(df)
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
table(nc$Best.n[1,])
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
#K-means cluster analysis
set.seed(1234)
fit.km <- kmeans(df, 3, nstart=25)
fit.km$size
fit.km$centers
aggregate(wine[-1], by=list(cluster=fit.km$cluster), mean)
# So how well did the K-means clustering uncover the actual structure of the data contained in the Type variable?
ct.km <- table(wine$Type, fit.km$cluster)
ct.km
ct.km
head(wine)
d <- data.frame()
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(2, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(3, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(4, each=20))))
# have a look...this looks easy enough
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d -- easy clusters')
# perform clustering
result1 <- kmeans(d[,1:2], 4)
# here are the results...note the algorithm found clusters whose means are close to the true means of our synthetic clusters
result1
# plot results...we are looking good
d$cluster1 <- as.factor(result1$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster1)) + ggtitle('kmeans result1 -- success!\n(k=4)')
# suppose we repeat these steps...what do you expect to happen?
result2 <- kmeans(d[,1:2], 4)
# notice that the fit got worse!
# (eg, large decrease in between_SS / total_SS...also cluster means are not as good as before)
result2
# and this scatterplot shows that something is obviously not right...what happened?
d$cluster2 <- as.factor(result2$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster2)) + ggtitle('kmeans result2 -- trouble\n(k=4)')
# this instability is a result of the random initial seeds that the clustering algorithm uses
# if two initial seeds begin in the same cluster, then the algorithm will have difficulty finding all the clusters
# (in particular, the cluster which doesn't contain an initial seed will be difficult to identify)
# (note that in any case, the algorithm will still return exactly as many clusters as you asked it to!)
# so how can we create a more stable clustering fit? by repeating the fit several times and taking an average
# (this is effectively an ensemble clustering technique...we will talk about ensemble methods in more detail later)
result3 <- kmeans(d[,1:2], 4, nstart=10)
d$cluster3 <- as.factor(result3$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster3)) + ggtitle('kmeans result3 -- stable convergence\n(k=4, nstart=10)')
#
# what happens if we introduce a new length scale into the problem? how many clusters are in the dataset now?
d2 <- rbind(d[,1:3], data.frame(x=1000+rnorm(20,0,50), y=1000+rnorm(20,0,50), label=as.factor(rep(5, each=20))))
ggplot(d2, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d2 -- multiple length scales')
# as you can see, things go haywire...recall that clustering results are kind of a heuristic
# (in particular, not invariant to a change in units!)
result4 <- kmeans(d2[,1:2], 5, nstart=10)
d2$cluster4 <- as.factor(result4$cluster)
ggplot(d2, aes(x=x, y=y)) + geom_point(aes(colour=cluster4)) + ggtitle('kmeans result4 -- trouble\n(k=5, nstart=10)')
#
# now let's try k-means clustering with the iris dataset
iris.result <- kmeans(iris[,1:4], 3)
# look at clustering results...you can already tell something is up
iris.result$cluster
# combine clustering results with input data (as factor)
# let's look at the scatterplots of clustering results & true labels together (using package gridExtra)
# first install this guy
install.packages('gridExtra')
install.packages("gridExtra")
library(gridExtra)
# now create our two scatterplots...note that ggplot returns an *object* which can be stored!
iris2 <- cbind(iris, cluster=as.factor(iris.result$cluster))
p1 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=cluster)) + ggtitle('clustering results')
p2 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=Species)) + ggtitle('true labels')
# so what is going on here?
grid.arrange(p1, p2)
head(iris)
result4
results3
result3
result3 <- kmeans(d[,1:2], 4, nstart=100)
result3
??kmeans
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
data(wine, package="rattle")
head(wine)
#Standardise data
df <- scale(wine[-1])
head(df)
wssplot(df)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
data(wine, package="rattle")
head(wine)
#Standardise data
df <- scale(wine[-1])
head(df)
#Determine number of clusters
wssplot(df)
var(wine[,2])
var(wine[2,])
var(wine[,2])
head(wine)
var(wine[,2])
tail(wine)
wssplot(df)
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
table(nc$Best.n[1,])
??NbClus
ncWard <- NbClust(df, min.nc=2, max.nc=15, method="Ward")
ncWard <- NbClust(df, min.nc=2, max.nc=15, method="ward")
table(nc$Best.n[1,])
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
table(ncWard$Best.n[1,])
barplot(table(ncWard$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
wssplot(df)
nc
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
var(wine)
fit.km <- kmeans(df, 3, nstart=25)
fit.km$size
fit.km$centers
aggregate(wine[-1], by=list(cluster=fit.km$cluster), mean)
install.packages("tcltk")
setwd('/Users/rjohnson/Documents/DS/DS_SYD_1/data//Networks & Graphs')
load("termDocMatrix.rdata")
# inspect part of the matrix
termDocMatrix[5:10,1:20]
# change it to a Boolean matrix
termDocMatrix[termDocMatrix>=1] <- 1
# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
termDocMatrix <- as.matrix(termDocMatrix)
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
plot(g, layout=layout.kamada.kawai)
#tkplot(g, layout=layout.kamada.kawai)
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)
setwd('/Users/rjohnson/Documents/DS/DataScience/FinalProject')
basics = read.delim('sql_queries/request_info.txt', sep="|")
head(basics)
plot(basics)
